# üìä –†–£–ö–û–í–û–î–°–¢–í–û –ü–û –ü–†–ê–í–ò–õ–¨–ù–û–ô –ì–ï–ù–ï–†–ê–¶–ò–ò –î–ê–ù–ù–´–• –í TEACHAI

## üö® –ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö

### ‚ùå –ù–ï –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï (–≤—ã–∑—ã–≤–∞–µ—Ç FileNotFoundError)
```python
# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ - –ó–ê–ü–†–ï–©–ï–ù–û!
data = pd.read_csv('data.csv')           # ‚ùå FileNotFoundError!
data = pd.read_excel('dataset.xlsx')     # ‚ùå FileNotFoundError!
data = pd.read_json('data.json')         # ‚ùå FileNotFoundError!
data = pd.read_sql('SELECT * FROM table') # ‚ùå ConnectionError!

# –û—Ç–∫—Ä—ã—Ç–∏–µ —Ñ–∞–π–ª–æ–≤ - –ó–ê–ü–†–ï–©–ï–ù–û!
with open('data.txt', 'r') as f:         # ‚ùå FileNotFoundError!
    data = f.read()

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º - –ó–ê–ü–†–ï–©–ï–ù–û!
data = np.load('array.npy')              # ‚ùå FileNotFoundError!
data = pickle.load(open('model.pkl', 'rb')) # ‚ùå FileNotFoundError!
```

## ‚úÖ –†–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö

### 1. **NumPy –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**
```python
import numpy as np

# –°–ª—É—á–∞–π–Ω—ã–µ —á–∏—Å–ª–∞
np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
random_numbers = np.random.randn(1000)  # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
uniform_numbers = np.random.uniform(0, 1, 1000)  # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
integers = np.random.randint(1, 100, 1000)  # –°–ª—É—á–∞–π–Ω—ã–µ —Ü–µ–ª—ã–µ

# –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–æ–≤
X = np.random.randn(1000, 10)  # 1000 –æ–±—Ä–∞–∑—Ü–æ–≤, 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
y = np.random.randint(0, 2, 1000)  # –ë–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤

# –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
feature1 = np.random.randn(1000) * 2 + 5
feature2 = np.random.randn(1000) * 1.5 + 3
target = 2 * feature1 + 1.5 * feature2 + np.random.randn(1000) * 0.5
```

### 2. **Sklearn –¥–ª—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤**
```python
from sklearn.datasets import load_iris, make_classification, make_regression

# –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
X_class, y_class = make_classification(
    n_samples=1000, 
    n_features=20, 
    n_informative=15,
    n_redundant=5,
    random_state=42
)

X_reg, y_reg = make_regression(
    n_samples=1000, 
    n_features=10, 
    noise=0.1,
    random_state=42
)

# –î—Ä—É–≥–∏–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
from sklearn.datasets import load_breast_cancer, load_digits, load_wine
cancer = load_breast_cancer()
digits = load_digits()
wine = load_wine()
```

### 3. **Pandas –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü**
```python
import pandas as pd
import numpy as np

# –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –∏–∑ –º–∞—Å—Å–∏–≤–æ–≤
np.random.seed(42)
n_samples = 1000

data = pd.DataFrame({
    'feature1': np.random.randn(n_samples) * 2 + 5,
    'feature2': np.random.randn(n_samples) * 1.5 + 3,
    'feature3': np.random.randint(0, 10, n_samples),
    'target': np.random.randint(0, 2, n_samples)
})

# –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
dates = pd.date_range('2023-01-01', periods=1000, freq='D')
time_series = pd.DataFrame({
    'date': dates,
    'value': np.random.randn(1000).cumsum(),
    'trend': np.arange(1000) * 0.1
})

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
categories = ['A', 'B', 'C', 'D']
categorical_data = pd.DataFrame({
    'category': np.random.choice(categories, 1000),
    'value': np.random.randn(1000)
})
```

### 4. **–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Python**
```python
# –°–ø–∏—Å–∫–∏ –∏ —Å–ª–æ–≤–∞—Ä–∏
numbers = list(range(1, 101))
squares = [x**2 for x in numbers]
even_numbers = [x for x in numbers if x % 2 == 0]

# –°–ª–æ–≤–∞—Ä–∏
student_data = {
    'names': ['–ò–≤–∞–Ω', '–ú–∞—Ä–∏—è', '–ü–µ—Ç—Ä', '–ê–Ω–Ω–∞'],
    'ages': [20, 22, 21, 23],
    'grades': [4.5, 4.8, 4.2, 4.9]
}

# –°—Ç—Ä–æ–∫–∏
text_data = "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. " * 100
words = text_data.split()
word_counts = {word: words.count(word) for word in set(words)}
```

## üéØ –®–∞–±–ª–æ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á

### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
```python
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification

# –°–ø–æ—Å–æ–± 1: –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    n_classes=3,
    random_state=42
)

# –°–ø–æ—Å–æ–± 2: –†—É—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
np.random.seed(42)
n_samples = 1000
n_features = 10

# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
X = np.random.randn(n_samples, n_features)

# –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
y = np.zeros(n_samples)
y[X[:, 0] + X[:, 1] > 0] = 1
y[X[:, 0] + X[:, 1] > 2] = 2

# –°–æ–∑–¥–∞–µ–º DataFrame
data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])
data['target'] = y
```

### –†–µ–≥—Ä–µ—Å—Å–∏—è
```python
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
import numpy as np
import pandas as pd

np.random.seed(42)
n_samples = 1000

# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
feature1 = np.random.randn(n_samples) * 2 + 5
feature2 = np.random.randn(n_samples) * 1.5 + 3
feature3 = np.random.uniform(0, 10, n_samples)

# –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å –ª–∏–Ω–µ–π–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å—é
target = 2 * feature1 + 1.5 * feature2 - 0.8 * feature3 + np.random.randn(n_samples) * 0.5

# –°–æ–∑–¥–∞–µ–º DataFrame
data = pd.DataFrame({
    'feature1': feature1,
    'feature2': feature2,
    'feature3': feature3,
    'target': target
})
```

### –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
```python
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
import numpy as np
import pandas as pd
from sklearn.datasets import make_blobs

# –°–ø–æ—Å–æ–± 1: –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
X, y_true = make_blobs(
    n_samples=1000,
    centers=4,
    cluster_std=1.0,
    random_state=42
)

# –°–ø–æ—Å–æ–± 2: –†—É—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
np.random.seed(42)
n_samples = 1000

# –°–æ–∑–¥–∞–µ–º —Ü–µ–Ω—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
centers = np.array([
    [0, 0],
    [5, 5],
    [0, 5],
    [5, 0]
])

# –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤–æ–∫—Ä—É–≥ —Ü–µ–Ω—Ç—Ä–æ–≤
X = []
y_true = []

for i, center in enumerate(centers):
    cluster_points = np.random.randn(n_samples // 4, 2) * 0.5 + center
    X.extend(cluster_points)
    y_true.extend([i] * (n_samples // 4))

X = np.array(X)
y_true = np.array(y_true)

# –°–æ–∑–¥–∞–µ–º DataFrame
data = pd.DataFrame(X, columns=['x', 'y'])
data['cluster'] = y_true
```

## üöÄ –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

### 1. **–í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏**
```python
import numpy as np
np.random.seed(42)  # –§–∏–∫—Å–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
```

### 2. **–°–æ–∑–¥–∞–≤–∞–π—Ç–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**
```python
# ‚úÖ –•–æ—Ä–æ—à–æ - —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
feature1 = np.random.randn(1000) * 2 + 5
feature2 = np.random.randn(1000) * 1.5 + 3
target = 2 * feature1 + 1.5 * feature2 + np.random.randn(1000) * 0.5

# ‚ùå –ü–ª–æ—Ö–æ - —Å–ª—É—á–∞–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
feature1 = np.random.randn(1000)
feature2 = np.random.randn(1000)
target = np.random.randn(1000)  # –ù–µ—Ç —Å–≤—è–∑–∏ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
```

### 3. **–î–æ–±–∞–≤–ª—è–π—Ç–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö**
```python
# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
# 1000 –æ–±—Ä–∞–∑—Ü–æ–≤, 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, 3 –∫–ª–∞—Å—Å–∞
np.random.seed(42)
X, y = make_classification(
    n_samples=1000,
    n_features=10,
    n_informative=8,
    n_redundant=2,
    n_classes=3,
    random_state=42
)
```

### 4. **–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**
```python
# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
print(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö: X={X.shape}, y={y.shape}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
unique, counts = np.unique(y, return_counts=True)
print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
for class_idx, count in zip(unique, counts):
    print(f"  –ö–ª–∞—Å—Å {class_idx}: {count} –æ–±—Ä–∞–∑—Ü–æ–≤")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
print(f"  –°—Ä–µ–¥–Ω–µ–µ: {X.mean(axis=0)}")
print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {X.std(axis=0)}")
```

## üìã –ß–µ–∫-–ª–∏—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

–ü–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –ø—Ä–∏–º–µ—Ä–∞ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:

- [ ] –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `pd.read_csv()`, `pd.read_excel()`, `open()`
- [ ] –ù–ï –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—É—Ç–∏ –∫ –≤–Ω–µ—à–Ω–∏–º —Ñ–∞–π–ª–∞–º
- [ ] –î–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é numpy.random –∏–ª–∏ sklearn.datasets
- [ ] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `np.random.seed()` –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
- [ ] –°–æ–∑–¥–∞—é—Ç—Å—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
- [ ] –î–æ–±–∞–≤–ª–µ–Ω—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
- [ ] –ü—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –∫–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

## üéâ –†–µ–∑—É–ª—å—Ç–∞—Ç

–°–ª–µ–¥—É—è —ç—Ç–æ–º—É —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É, –≤—ã –∏–∑–±–µ–∂–∏—Ç–µ:
- ‚ùå `FileNotFoundError: No such file or directory: 'data.csv'`
- ‚ùå `FileNotFoundError: No such file or directory: 'dataset.xlsx'`
- ‚ùå `ConnectionError` –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö

–ò –ø–æ–ª—É—á–∏—Ç–µ:
- ‚úÖ –°–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- ‚úÖ –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- ‚úÖ –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
- ‚úÖ –õ–µ–≥–∫–æ—Å—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ—Ç–ª–∞–¥–∫–∏

**–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö - –∑–∞–ª–æ–≥ —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤!** üêç‚ú® 